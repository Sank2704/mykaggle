{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"* The following kernel is based on the dataset available from [data](https://www.kaggle.com/rohanrao/nifty50-stock-market-data)\n* This dataset is an extensive collection of Stock Market data of Nifty Stocks aggregated from (2000 - 2019) [Nifty 50](https://en.wikipedia.org/wiki/NIFTY_50)  \n* The NIFTY 50 index National Stock Exchange of India's benchmark broad based stock market index for the Indian equity market. Full form of NIFTY is National Index Fifty.\n"},{"metadata":{},"cell_type":"markdown","source":"***What is the purpose of this Kernel***"},{"metadata":{},"cell_type":"markdown","source":"* This kernel is an attempt to perform analysis on how the stock data has been behaving over the years across various stocks\n* Also we will look at few visualizations which will help us viewing the data as a graph instead of as codes"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install pmdarima\n#Make sure you have enabled internet while running this inside Kaggle Kernel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Importing the most frequent libraries used\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np  # linear algebra\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#setting figure size\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 20,10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for normalizing data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Understanding the columns available in the dataset**"},{"metadata":{},"cell_type":"markdown","source":"* Date - The date at which the data is reccorded, typically wont have Weekends and National/Public holidays\n* Symbol - The short text by which the Company/Stock is identified in Nifty (basically kind of primary key)\n* Series - This indicates which series it belongs to [Series](https://help.tradesmartonline.in/what-does-eq-and-be-series-stand-for-in-nse/)\n* Prev Close - The Closing Price on the preceeding time period\n* Open - The opening price on the stock \n* High - The highest price of the stock on that particular time period \n* Low - The lowest price of the stock on that particular time period\n* Last - [What is Last](https://www.sapling.com/8101485/last-mean-stocks)\n* Close - The closing price of the stock\n* Vwap - The volume weighted average price (VWAP) is a trading benchmark used by traders that gives the average price a security has traded at throughout the day, based on both volume and price. [What is Vwap](https://www.investopedia.com/terms/v/vwap.asp)\n* Volume - Volume is the number of shares or contracts traded in a security or an entire market during a given period of time. [What is Volume](https://www.investopedia.com/terms/v/volume.asp)\n* Turnover - Share turnover is a measure of stock liquidity calculated by dividing the total number of shares traded over a period by the average number of shares outstanding for the period [What is Turnover](https://www.investopedia.com/terms/s/shareturnover.asp)\n* Trades - Trade in stock markets means the transfer (in exchange for money) \n* Delieverable Volume - \n* % Delivarable - \n\n1. (**Please let me know if some columns have been misunderstood, and all these data have been aggregated from Internet as I am not completely aware of the Stocks world**)"},{"metadata":{},"cell_type":"markdown","source":"***How to analyse this data ?***\n\nAs seen from the data we know that there is a time dependent column and hence this is best suited for Time Series analysis, which means in our analysis time period will be the primary point in the analysis\n\nInspiration for this kernel (https://www.kaggle.com/rohanrao/a-modern-time-series-tutorial) and (https://www.kaggle.com/parulpandey/getting-started-with-time-series-using-pandas)"},{"metadata":{},"cell_type":"markdown","source":"### **Data Preparation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will use one dataset from the above list to perform our analysis (Maruti) \ndata = pd.read_csv(\"/kaggle/input/nifty50-stock-market-data/MARUTI.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will creat a empty dataframe to store all our prediction results\nplot_df = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape\n#4098 rows and 15 columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This will show us the what are the data columns and its data type available for analysis\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Not much, but still we seem to have null values as shown below\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can take the Vwap as our target variable "},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will drop this column as we are not going to use this and it has considerable amount of null values \n#We will also drop the null values\ndata.drop(['Trades'], axis=1,inplace = True)\ndata.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will set the Index to the date column availabe as it will be best suited in this secnario\ndata.set_index(\"Date\", drop=False, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Throught this kernel we will use yticks as (100,10000,1000) which gives us a common frequency of 1000 and we will compare each plots arrived"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.VWAP.plot()\n#Shows as increasing trend over the time\nplt.yticks(np.arange(100, 10000, 1000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Open','Close','VWAP','High','Low']].plot()\nplt.yticks(np.arange(100, 10000, 1000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets visualize the correlation among the data\ncorr = data.corr()\nsns.heatmap(corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Date = pd.to_datetime(data.Date, format=\"%Y-%m-%d\")\ndata[\"month\"] = data.Date.dt.month\ndata[\"week\"] = data.Date.dt.week\ndata[\"day\"] = data.Date.dt.day\ndata[\"day_of_week\"] = data.Date.dt.dayofweek\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split is not random, as we are dependent on time for the analysis\ndata_train = data[data.Date < \"2019\"]\ndata_valid = data[data.Date >= \"2019\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **ARIMA**\nhttps://www.analyticsvidhya.com/blog/2018/08/auto-arima-time-series-modeling-python-r/"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pmdarima import auto_arima\n\nmodel_ARIMA = auto_arima(data_train.VWAP,trace=True, start_p=1, start_q=1,max_p=3, max_q=3, \n                   m=12,start_P=0, seasonal=True,d=1, D=1,error_action='ignore',suppress_warnings=True)\nmodel_ARIMA.fit(data_train.VWAP)\n\nforecast_ARIMA = model_ARIMA.predict(n_periods=len(data_valid))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df['VWAP'] = data_valid['VWAP']\nplot_df['Forecast_ARIMAX'] = forecast_ARIMA\nplot_df[[\"VWAP\", \"Forecast_ARIMAX\"]].plot()\nplt.yticks(np.arange(100, 10000, 1000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **KNN**\nhttps://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing libraries\nfrom sklearn import neighbors\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = data_train.drop(['VWAP','Date','Symbol','Series'], axis=1)\ny_train = data_train['VWAP']\nx_valid = data_valid.drop(['VWAP','Date','Symbol','Series'], axis=1)\ny_valid = data_valid['VWAP']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaling data\nx_train_scaled = scaler.fit_transform(x_train)\nx_train = pd.DataFrame(x_train_scaled)\nx_valid_scaled = scaler.fit_transform(x_valid)\nx_valid = pd.DataFrame(x_valid_scaled)\n\n#using gridsearch to find the best parameter\nparams = {'n_neighbors':[2,3,4,5,6,7,8,9]}\nknn = neighbors.KNeighborsRegressor()\nmodel_knn = GridSearchCV(knn, params, cv=5)\n\n#fit the model and make predictions\nmodel_knn.fit(x_train,y_train)\nforecast_knn = model_knn.predict(x_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df[\"Forecast_KNN\"] = forecast_knn\nplot_df[[\"VWAP\", \"Forecast_KNN\"]].plot()\nplt.yticks(np.arange(100, 10000, 1000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Linear Regression**\nhttps://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/"},{"metadata":{"trusted":true},"cell_type":"code","source":"#implement linear regression\nfrom sklearn.linear_model import LinearRegression\nmodel_lin = LinearRegression()\nmodel_lin.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_lin = model_lin.predict(x_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df[\"Forecast_lin\"] = preds_lin\nplot_df[[\"VWAP\", \"Forecast_lin\"]].plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Prophet**\nhttps://facebook.github.io/prophet/docs/quick_start.html#python-api"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet import Prophet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will use one dataset from the above list to perform our analysis (Maruti) \ndata_prophet = pd.read_csv(\"/kaggle/input/nifty50-stock-market-data/MARUTI.csv\")\ndata_prophet.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_p = data_prophet[data_prophet.Date < \"2019\"]\ndata_valid_p = data_prophet[data_prophet.Date >= \"2019\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit the model\nmodel_fbp = Prophet()\nmodel_fbp.fit(data_train_p[[\"Date\", \"VWAP\"]].rename(columns={\"Date\": \"ds\", \"VWAP\": \"y\"}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast_prophet = model_fbp.predict(data_valid_p[[\"Date\", \"VWAP\"]].rename(columns={\"Date\": \"ds\"}))\npreds_prophet = forecast_prophet.yhat.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df[\"Forecast_prophet\"] = preds_prophet\nplot_df[[\"VWAP\", \"Forecast_prophet\"]].plot()\nplt.yticks(np.arange(100, 10000, 1000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data = pd.read_csv(\"/kaggle/input/nifty50-stock-market-data/MARUTI.csv\")\nnew_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Below feature engineering I have learned from the kernel (https://www.kaggle.com/rohanrao/a-modern-time-series-tutorial)\n* Credits to : Vopani"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data.reset_index(drop=True, inplace=True)\nlag_features = [\"High\", \"Low\", \"Volume\", \"Turnover\", \"Trades\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\nnew_data_rolled_3d = new_data[lag_features].rolling(window=window1, min_periods=0)\nnew_data_rolled_7d = new_data[lag_features].rolling(window=window2, min_periods=0)\nnew_data_rolled_30d = new_data[lag_features].rolling(window=window3, min_periods=0)\n\nnew_data_mean_3d = new_data_rolled_3d.mean().shift(1).reset_index().astype(np.float32)\nnew_data_mean_7d = new_data_rolled_7d.mean().shift(1).reset_index().astype(np.float32)\nnew_data_mean_30d = new_data_rolled_30d.mean().shift(1).reset_index().astype(np.float32)\n\nnew_data_std_3d = new_data_rolled_3d.std().shift(1).reset_index().astype(np.float32)\nnew_data_std_7d = new_data_rolled_7d.std().shift(1).reset_index().astype(np.float32)\nnew_data_std_30d = new_data_rolled_30d.std().shift(1).reset_index().astype(np.float32)\n\nfor feature in lag_features:\n    new_data[f\"{feature}_mean_lag{window1}\"] = new_data_mean_3d[feature]\n    new_data[f\"{feature}_mean_lag{window2}\"] = new_data_mean_7d[feature]\n    new_data[f\"{feature}_mean_lag{window3}\"] = new_data_mean_30d[feature]\n    \n    new_data[f\"{feature}_std_lag{window1}\"] = new_data_std_3d[feature]\n    new_data[f\"{feature}_std_lag{window2}\"] = new_data_std_7d[feature]\n    new_data[f\"{feature}_std_lag{window3}\"] = new_data_std_30d[feature]\n\nnew_data.fillna(new_data.mean(), inplace=True)\n\nnew_data.set_index(\"Date\", drop=False, inplace=True)\nnew_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data.Date = pd.to_datetime(new_data.Date, format=\"%Y-%m-%d\")\nnew_data[\"month\"] = new_data.Date.dt.month\nnew_data[\"week\"] = new_data.Date.dt.week\nnew_data[\"day\"] = new_data.Date.dt.day\nnew_data[\"day_of_week\"] = new_data.Date.dt.dayofweek\nnew_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data_train = new_data[new_data.Date < \"2019\"]\nnew_data_valid = new_data[new_data.Date >= \"2019\"]\n\nexogenous_features = [\"High_mean_lag3\", \"High_std_lag3\", \"Low_mean_lag3\", \"Low_std_lag3\",\n                      \"Volume_mean_lag3\", \"Volume_std_lag3\", \"Turnover_mean_lag3\",\n                      \"Turnover_std_lag3\", \"Trades_mean_lag3\", \"Trades_std_lag3\",\n                      \"High_mean_lag7\", \"High_std_lag7\", \"Low_mean_lag7\", \"Low_std_lag7\",\n                      \"Volume_mean_lag7\", \"Volume_std_lag7\", \"Turnover_mean_lag7\",\n                      \"Turnover_std_lag7\", \"Trades_mean_lag7\", \"Trades_std_lag7\",\n                      \"High_mean_lag30\", \"High_std_lag30\", \"Low_mean_lag30\", \"Low_std_lag30\",\n                      \"Volume_mean_lag30\", \"Volume_std_lag30\", \"Turnover_mean_lag30\",\n                      \"Turnover_std_lag30\", \"Trades_mean_lag30\", \"Trades_std_lag30\",\n                      \"month\", \"week\", \"day\", \"day_of_week\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_fbp_features = Prophet()\nfor feature in exogenous_features:\n    model_fbp_features.add_regressor(feature)\n\nmodel_fbp_features.fit(new_data_train[[\"Date\", \"VWAP\"] + exogenous_features].rename(columns={\"Date\": \"ds\", \"VWAP\": \"y\"}))\n\nforecast_prophet_features = model_fbp_features.predict(new_data_valid[[\"Date\", \"VWAP\"] + exogenous_features].rename(columns={\"Date\": \"ds\"}))\nplot_df[\"Forecast_Prophet_features\"] = forecast_prophet_features.yhat.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df[[\"VWAP\",\"Forecast_Prophet_features\"]].plot()\nplt.yticks(np.arange(100, 10000, 1000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Overall Comparision of various timeseries models\nplot_df[[\"VWAP\",\"Forecast_ARIMAX\" , \"Forecast_KNN\" , \"Forecast_lin\" , \"Forecast_prophet\" , \"Forecast_Prophet_features\"]].plot()\nplt.yticks(np.arange(100, 10000, 1000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Learned from Kernels \n* https://www.kaggle.com/rohanrao/a-modern-time-series-tutorial\n* Blog : https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/\n* Like this Kernel ? Please help me in getting motivated by upvoting\n* Have a suggestion ? or found a mistake in code or in any of the terms ? or have a better way to improve this ?? \n* Please let me know in the comments"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}